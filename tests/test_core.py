"""
Comprehensive test suite for the core compositional prompting library.
Tests all core functionality including providers, fluid API, and compositional methods.
"""

import pytest
from unittest.mock import Mock, patch, MagicMock
from dataclasses import FrozenInstanceError
from typing import List
import random
import numpy as np

from compositional_prompting import (
    # Providers
    LLMProvider,
    MockLLMProvider,
    OpenAIProvider,
    AnthropicProvider,
    OllamaProvider,

    # Core Enums
    CognitiveOperation,
    FocusAspect,
    ReasoningStyle,
    ConnectionType,
    OutputFormat,

    # Extended Enums
    MetaStrategy,
    ConfidenceLevel,
    ReasoningDepth,

    # Main API
    ComposingPrompt
)


class TestLLMProviders:
    """Test LLM provider implementations"""

    def test_mock_provider_generates_response(self):
        """Test that mock provider generates expected response"""
        provider = MockLLMProvider()
        prompt = "Test prompt for mock provider"
        response = provider.generate(prompt, max_tokens=100, temperature=0.7)

        assert "[MOCK LLM Response" in response
        assert "Test prompt" in response
        assert provider.get_provider_name() == "MockLLM"

    def test_mock_provider_handles_long_prompt(self):
        """Test mock provider with long prompts"""
        provider = MockLLMProvider()
        prompt = "x" * 100  # Long prompt
        response = provider.generate(prompt)

        assert "[MOCK LLM Response" in response
        assert len(response) < 100  # Should truncate

    def test_openai_provider_initialization(self):
        """Test OpenAI provider initialization"""
        # Mock the openai module and its components
        mock_openai_module = Mock()
        mock_openai_class = Mock()
        mock_client = Mock()

        mock_openai_module.OpenAI = mock_openai_class
        mock_openai_class.return_value = mock_client

        with patch.dict('sys.modules', {'openai': mock_openai_module}):
            provider = OpenAIProvider(api_key="test-key", model="gpt-4")

            assert provider.model == "gpt-4"
            assert provider.get_provider_name() == "OpenAI-gpt-4"
            mock_openai_class.assert_called_once_with(api_key="test-key")

    def test_openai_provider_generate(self):
        """Test OpenAI provider generation"""
        # Mock the openai module and its components
        mock_openai_module = Mock()
        mock_openai_class = Mock()
        mock_client = Mock()
        mock_response = Mock()

        mock_response.choices = [Mock(message=Mock(content="Generated text"))]
        mock_client.chat.completions.create.return_value = mock_response
        mock_openai_class.return_value = mock_client
        mock_openai_module.OpenAI = mock_openai_class

        with patch.dict('sys.modules', {'openai': mock_openai_module}):
            provider = OpenAIProvider(api_key="test-key")
            result = provider.generate("Test prompt", max_tokens=50, temperature=0.5)

            assert result == "Generated text"
            mock_client.chat.completions.create.assert_called_once()
            call_args = mock_client.chat.completions.create.call_args
            assert call_args.kwargs['max_tokens'] == 50
            assert call_args.kwargs['temperature'] == 0.5

    def test_openai_provider_import_error(self):
        """Test OpenAI provider handles missing package"""
        with patch.dict('sys.modules', {'openai': None}):
            with pytest.raises(ImportError) as exc_info:
                OpenAIProvider(api_key="test-key")
            assert "openai package not installed" in str(exc_info.value)

    def test_anthropic_provider_initialization(self):
        """Test Anthropic provider initialization"""
        # Mock the anthropic module and its components
        mock_anthropic_module = Mock()
        mock_anthropic_class = Mock()
        mock_client = Mock()

        mock_anthropic_module.Anthropic = mock_anthropic_class
        mock_anthropic_class.return_value = mock_client

        with patch.dict('sys.modules', {'anthropic': mock_anthropic_module}):
            provider = AnthropicProvider(api_key="test-key", model="claude-3-opus")

            assert provider.model == "claude-3-opus"
            assert provider.get_provider_name() == "Anthropic-claude-3-opus"
            mock_anthropic_class.assert_called_once_with(api_key="test-key")

    def test_anthropic_provider_generate(self):
        """Test Anthropic provider generation"""
        # Mock the anthropic module and its components
        mock_anthropic_module = Mock()
        mock_anthropic_class = Mock()
        mock_client = Mock()
        mock_response = Mock()
        mock_response.content = [Mock(text="Generated by Claude")]
        mock_client.messages.create.return_value = mock_response
        mock_anthropic_class.return_value = mock_client
        mock_anthropic_module.Anthropic = mock_anthropic_class

        with patch.dict('sys.modules', {'anthropic': mock_anthropic_module}):
            provider = AnthropicProvider(api_key="test-key")
            result = provider.generate("Test prompt", max_tokens=100, temperature=0.8)

            assert result == "Generated by Claude"
            mock_client.messages.create.assert_called_once()
            call_args = mock_client.messages.create.call_args
            assert call_args.kwargs['max_tokens'] == 100
            assert call_args.kwargs['temperature'] == 0.8

    def test_anthropic_provider_import_error(self):
        """Test Anthropic provider handles missing package"""
        with patch.dict('sys.modules', {'anthropic': None}):
            with pytest.raises(ImportError) as exc_info:
                AnthropicProvider(api_key="test-key")
            assert "anthropic package not installed" in str(exc_info.value)

    @patch('requests.Session')
    def test_ollama_provider_initialization(self, mock_session_class):
        """Test Ollama provider initialization"""
        mock_session = Mock()
        mock_session_class.return_value = mock_session

        provider = OllamaProvider(model="llama2", base_url="http://localhost:11434")

        assert provider.model == "llama2"
        assert provider.base_url == "http://localhost:11434"
        assert provider.get_provider_name() == "Ollama-llama2"

    @patch('requests.Session')
    def test_ollama_provider_generate(self, mock_session_class):
        """Test Ollama provider generation"""
        mock_session = Mock()
        mock_response = Mock()
        mock_response.json.return_value = {"response": "Generated locally"}
        mock_session.post.return_value = mock_response
        mock_session_class.return_value = mock_session

        provider = OllamaProvider()
        result = provider.generate("Test prompt", max_tokens=200, temperature=0.6)

        assert result == "Generated locally"
        mock_session.post.assert_called_once()
        call_args = mock_session.post.call_args
        json_data = call_args.kwargs['json']
        assert json_data['model'] == "llama2"
        assert json_data['prompt'] == "Test prompt"
        assert json_data['options']['num_predict'] == 200
        assert json_data['options']['temperature'] == 0.6

    def test_ollama_provider_import_error(self):
        """Test Ollama provider handles missing package"""
        with patch.dict('sys.modules', {'requests': None}):
            with pytest.raises(ImportError) as exc_info:
                OllamaProvider()
            assert "requests package not installed" in str(exc_info.value)


class TestEnums:
    """Test enum definitions"""

    def test_cognitive_operation_values(self):
        """Test CognitiveOperation enum values"""
        assert CognitiveOperation.DECOMPOSE.value == "decompose"
        assert CognitiveOperation.ANALYZE.value == "analyze"
        assert CognitiveOperation.GENERATE.value == "generate"
        assert CognitiveOperation.VERIFY.value == "verify"
        assert CognitiveOperation.SYNTHESIZE.value == "synthesize"
        assert CognitiveOperation.ABSTRACT.value == "abstract"

    def test_focus_aspect_values(self):
        """Test FocusAspect enum values"""
        assert FocusAspect.STRUCTURE.value == "structure"
        assert FocusAspect.CONSTRAINTS.value == "constraints"
        assert FocusAspect.PATTERNS.value == "patterns"
        assert FocusAspect.EDGE_CASES.value == "edge_cases"

    def test_reasoning_style_values(self):
        """Test ReasoningStyle enum values"""
        assert ReasoningStyle.SYSTEMATIC.value == "systematic"
        assert ReasoningStyle.CREATIVE.value == "creative"
        assert ReasoningStyle.CRITICAL.value == "critical"
        assert ReasoningStyle.FORMAL.value == "formal"
        assert ReasoningStyle.INTUITIVE.value == "intuitive"

    def test_connection_type_values(self):
        """Test ConnectionType enum values"""
        assert ConnectionType.THEREFORE.value == "therefore"
        assert ConnectionType.HOWEVER.value == "however"
        assert ConnectionType.BUILDING_ON.value == "building_on"
        assert ConnectionType.ALTERNATIVELY.value == "alternatively"

    def test_output_format_values(self):
        """Test OutputFormat enum values"""
        assert OutputFormat.STEPS.value == "steps"
        assert OutputFormat.LIST.value == "list"
        assert OutputFormat.CODE.value == "code"
        assert OutputFormat.MATHEMATICAL.value == "mathematical"

    def test_meta_strategy_values(self):
        """Test MetaStrategy enum values"""
        assert MetaStrategy.FORWARD_CHAINING.value == "forward_chaining"
        assert MetaStrategy.BACKWARD_CHAINING.value == "backward_chaining"
        assert MetaStrategy.CASE_BASED_REASONING.value == "case_based_reasoning"

    def test_confidence_level_values(self):
        """Test ConfidenceLevel enum values"""
        assert ConfidenceLevel.VERY_LOW.value == "very_low"
        assert ConfidenceLevel.HIGH.value == "high"
        assert ConfidenceLevel.CERTAIN.value == "certain"

    def test_reasoning_depth_values(self):
        """Test ReasoningDepth enum values"""
        assert ReasoningDepth.SURFACE.value == "surface"
        assert ReasoningDepth.DEEP.value == "deep"
        assert ReasoningDepth.PROFOUND.value == "profound"


class TestComposingPromptBasics:
    """Test basic ComposingPrompt functionality"""

    def test_initialization_with_default_provider(self):
        """Test initialization with default mock provider"""
        prompt = ComposingPrompt()
        assert isinstance(prompt._llm_provider, MockLLMProvider)

    def test_initialization_with_custom_provider(self):
        """Test initialization with custom provider"""
        custom_provider = MockLLMProvider()
        prompt = ComposingPrompt(_llm_provider=custom_provider)
        assert prompt._llm_provider is custom_provider

    def test_set_llm_provider(self):
        """Test setting LLM provider"""
        prompt = ComposingPrompt()
        new_provider = MockLLMProvider()
        result = prompt.set_llm_provider(new_provider)

        assert prompt._llm_provider is new_provider
        assert result is prompt  # Fluent API

    def test_problem_context_setting(self):
        """Test setting problem context"""
        prompt = ComposingPrompt()
        result = prompt.problem_context("Solve a complex optimization problem")

        assert prompt._problem_context == "Solve a complex optimization problem"
        assert result is prompt

    def test_base_prompt_setting(self):
        """Test setting base prompt"""
        prompt = ComposingPrompt()
        result = prompt.base_prompt("Initial prompt content")

        assert prompt._base_prompt == "Initial prompt content"
        assert result is prompt


class TestCoreCompositionalMethods:
    """Test core compositional methods"""

    def test_cognitive_op_with_enum(self):
        """Test setting cognitive operation with enum"""
        prompt = ComposingPrompt()
        result = prompt.cognitive_op(CognitiveOperation.DECOMPOSE)

        assert prompt._cognitive_op == CognitiveOperation.DECOMPOSE
        assert result is prompt

    def test_cognitive_op_with_string(self):
        """Test setting cognitive operation with string"""
        prompt = ComposingPrompt()
        result = prompt.cognitive_op("analyze")

        assert prompt._cognitive_op == CognitiveOperation.ANALYZE
        assert result is prompt

    def test_cognitive_op_invalid_string(self):
        """Test cognitive operation with invalid string"""
        prompt = ComposingPrompt()
        with pytest.raises(ValueError):
            prompt.cognitive_op("invalid_operation")

    def test_focus_with_enum(self):
        """Test setting focus aspect with enum"""
        prompt = ComposingPrompt()
        result = prompt.focus(FocusAspect.PATTERNS)

        assert prompt._focus == FocusAspect.PATTERNS
        assert result is prompt

    def test_focus_with_string(self):
        """Test setting focus aspect with string"""
        prompt = ComposingPrompt()
        result = prompt.focus("constraints")

        assert prompt._focus == FocusAspect.CONSTRAINTS
        assert result is prompt

    def test_style_with_enum(self):
        """Test setting reasoning style with enum"""
        prompt = ComposingPrompt()
        result = prompt.style(ReasoningStyle.CRITICAL)

        assert prompt._style == ReasoningStyle.CRITICAL
        assert result is prompt

    def test_style_with_string(self):
        """Test setting reasoning style with string"""
        prompt = ComposingPrompt()
        result = prompt.style("systematic")

        assert prompt._style == ReasoningStyle.SYSTEMATIC
        assert result is prompt

    def test_connect_with_enum(self):
        """Test setting connection type with enum"""
        prompt = ComposingPrompt()
        result = prompt.connect(ConnectionType.HOWEVER)

        assert prompt._connection == ConnectionType.HOWEVER
        assert result is prompt

    def test_connect_with_string(self):
        """Test setting connection type with string"""
        prompt = ComposingPrompt()
        result = prompt.connect("therefore")

        assert prompt._connection == ConnectionType.THEREFORE
        assert result is prompt

    def test_format_with_enum(self):
        """Test setting output format with enum"""
        prompt = ComposingPrompt()
        result = prompt.format(OutputFormat.CODE)

        assert prompt._output_format == OutputFormat.CODE
        assert result is prompt

    def test_format_with_string(self):
        """Test setting output format with string"""
        prompt = ComposingPrompt()
        result = prompt.format("steps")

        assert prompt._output_format == OutputFormat.STEPS
        assert result is prompt


class TestExtendedDimensions:
    """Test extended dimensional methods"""

    def test_meta_strategy_with_enum(self):
        """Test setting meta strategy with enum"""
        prompt = ComposingPrompt()
        result = prompt.meta_strategy(MetaStrategy.BACKWARD_CHAINING)

        assert prompt._meta_strategy == MetaStrategy.BACKWARD_CHAINING
        assert result is prompt

    def test_meta_strategy_with_string(self):
        """Test setting meta strategy with string"""
        prompt = ComposingPrompt()
        result = prompt.meta_strategy("forward_chaining")

        assert prompt._meta_strategy == MetaStrategy.FORWARD_CHAINING
        assert result is prompt

    def test_confidence_with_enum(self):
        """Test setting confidence level with enum"""
        prompt = ComposingPrompt()
        result = prompt.confidence(ConfidenceLevel.HIGH)

        assert prompt._confidence == ConfidenceLevel.HIGH
        assert result is prompt

    def test_confidence_with_string(self):
        """Test setting confidence level with string"""
        prompt = ComposingPrompt()
        result = prompt.confidence("medium")

        assert prompt._confidence == ConfidenceLevel.MEDIUM
        assert result is prompt

    def test_depth_with_enum(self):
        """Test setting reasoning depth with enum"""
        prompt = ComposingPrompt()
        result = prompt.depth(ReasoningDepth.DEEP)

        assert prompt._depth == ReasoningDepth.DEEP
        assert result is prompt

    def test_depth_with_string(self):
        """Test setting reasoning depth with string"""
        prompt = ComposingPrompt()
        result = prompt.depth("moderate")

        assert prompt._depth == ReasoningDepth.MODERATE
        assert result is prompt


class TestFluentAPI:
    """Test fluent API chaining"""

    def test_full_chain(self):
        """Test full method chaining"""
        prompt = (ComposingPrompt()
                 .cognitive_op(CognitiveOperation.ANALYZE)
                 .focus(FocusAspect.PATTERNS)
                 .style(ReasoningStyle.SYSTEMATIC)
                 .connect(ConnectionType.THEREFORE)
                 .format(OutputFormat.STEPS)
                 .meta_strategy(MetaStrategy.FORWARD_CHAINING)
                 .confidence(ConfidenceLevel.HIGH)
                 .depth(ReasoningDepth.DEEP))

        assert prompt._cognitive_op == CognitiveOperation.ANALYZE
        assert prompt._focus == FocusAspect.PATTERNS
        assert prompt._style == ReasoningStyle.SYSTEMATIC
        assert prompt._connection == ConnectionType.THEREFORE
        assert prompt._output_format == OutputFormat.STEPS
        assert prompt._meta_strategy == MetaStrategy.FORWARD_CHAINING
        assert prompt._confidence == ConfidenceLevel.HIGH
        assert prompt._depth == ReasoningDepth.DEEP

    def test_partial_chain(self):
        """Test partial method chaining"""
        prompt = (ComposingPrompt()
                 .cognitive_op("decompose")
                 .focus("structure")
                 .style("creative"))

        assert prompt._cognitive_op == CognitiveOperation.DECOMPOSE
        assert prompt._focus == FocusAspect.STRUCTURE
        assert prompt._style == ReasoningStyle.CREATIVE
        assert prompt._connection is None
        assert prompt._output_format is None


class TestLLMAugmentation:
    """Test LLM augmentation methods"""

    def test_llm_augment_basic(self):
        """Test basic LLM augmentation"""
        prompt = ComposingPrompt()
        result = prompt.llm_augment("Make this more specific")

        assert len(prompt._llm_augmentations) == 1
        aug = prompt._llm_augmentations[0]
        assert aug['type'] == 'augment'
        assert aug['instruction'] == "Make this more specific"
        assert isinstance(aug['provider'], MockLLMProvider)
        assert result is prompt

    def test_llm_augment_with_custom_provider(self):
        """Test LLM augmentation with custom provider"""
        custom_provider = MockLLMProvider()
        prompt = ComposingPrompt()
        result = prompt.llm_augment("Enhance this", provider=custom_provider)

        aug = prompt._llm_augmentations[0]
        assert aug['provider'] is custom_provider

    def test_llm_coherence_check(self):
        """Test enabling coherence check"""
        prompt = ComposingPrompt()
        result = prompt.llm_coherence_check()

        assert prompt._coherence_checks is True
        assert isinstance(prompt._coherence_provider, MockLLMProvider)
        assert result is prompt

    def test_llm_coherence_check_with_provider(self):
        """Test coherence check with custom provider"""
        custom_provider = MockLLMProvider()
        prompt = ComposingPrompt()
        result = prompt.llm_coherence_check(provider=custom_provider)

        assert prompt._coherence_provider is custom_provider

    def test_llm_add_examples_basic(self):
        """Test adding examples via LLM"""
        prompt = ComposingPrompt()
        result = prompt.llm_add_examples(n=3)

        assert len(prompt._llm_augmentations) == 1
        aug = prompt._llm_augmentations[0]
        assert aug['type'] == 'examples'
        assert aug['n'] == 3
        assert aug['domain'] is None
        assert aug['parallel'] is True
        assert result is prompt

    def test_llm_add_examples_with_domain(self):
        """Test adding domain-specific examples"""
        prompt = ComposingPrompt()
        result = prompt.llm_add_examples(n=5, domain="mathematics", parallel=False)

        aug = prompt._llm_augmentations[0]
        assert aug['domain'] == "mathematics"
        assert aug['parallel'] is False
        assert "mathematics" in aug['instruction']

    def test_llm_termination_terminal_patterns(self):
        """Test termination detection with terminal patterns"""
        prompt = ComposingPrompt()

        terminal_states = [
            "Final answer: 42",
            "Therefore, the answer is 42",
            "In conclusion, we have proven the theorem",
            "QED",
            "## Solution\nThe result is 42"
        ]

        for state in terminal_states:
            assert prompt.llm_termination(state) is True

    def test_llm_termination_non_terminal(self):
        """Test termination detection with non-terminal text"""
        prompt = ComposingPrompt()

        non_terminal_states = [
            "Let's continue with step 2",
            "Next, we need to consider",
            "Moving forward with the analysis"
        ]

        for state in non_terminal_states:
            # Should use LLM check or length fallback
            result = prompt.llm_termination(state)
            assert result is False  # Short text, not terminal

    def test_llm_termination_length_fallback(self):
        """Test termination detection fallback to length"""
        prompt = ComposingPrompt()
        long_text = "a" * 2500  # Over 2000 chars

        assert prompt.llm_termination(long_text) is True

    def test_llm_termination_with_provider(self):
        """Test termination detection with custom provider"""
        mock_provider = Mock(spec=LLMProvider)
        mock_provider.generate.return_value = "TERMINAL"

        prompt = ComposingPrompt()
        result = prompt.llm_termination("Some text", provider=mock_provider)

        assert result is True
        mock_provider.generate.assert_called_once()

    def test_rag_add_examples(self):
        """Test RAG example addition"""
        prompt = ComposingPrompt()
        result = prompt.rag_add_examples(n=10, similarity_threshold=0.9)

        assert len(prompt._context_additions) == 1
        assert "Retrieve 10 similar examples" in prompt._context_additions[0]
        assert "0.9" in prompt._context_additions[0]
        assert result is prompt

    def test_add_context(self):
        """Test adding context"""
        prompt = ComposingPrompt()
        result = prompt.add_context("Additional context information")

        assert "Additional context information" in prompt._context_additions
        assert result is prompt

    def test_multiple_augmentations(self):
        """Test multiple augmentations"""
        prompt = (ComposingPrompt()
                 .llm_augment("First augmentation")
                 .llm_add_examples(3)
                 .llm_augment("Second augmentation")
                 .add_context("Context 1")
                 .add_context("Context 2"))

        assert len(prompt._llm_augmentations) == 3
        assert len(prompt._context_additions) == 2